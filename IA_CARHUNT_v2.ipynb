{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5414771",
   "metadata": {},
   "source": [
    "# Rotina de Preparação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparação de dados para Machine Learning\n",
    "\n",
    "#!pip install mlxtend\n",
    "# !pip install featuretools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import featuretools as ft\n",
    "import random \n",
    "import time\n",
    "\n",
    "# data = dataframe para ser preparado\n",
    "# target_column_name = coluna-alvo\n",
    "# c_to_drop = lista de colunas que certamente não se relacionam com a Coluna alvo\n",
    "# columns_to_slice = lista de tuplas com 2 strings: ('Nome da colna que pode ser splitada', 'delimitador') para ennriquecer a análise\n",
    "\n",
    "# e.g.\n",
    "# df,\"value\",['index','hash_code','unsable_column'],[('description',' ')]\n",
    "\n",
    "def prepare_data_for_ML(o_data,target_column_name,c_to_drop,columns_to_slice):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # Lista com mapeamentos, para usar o label encoder\n",
    "    maps = []\n",
    "    \n",
    "    data = o_data\n",
    "\n",
    "    # Step 1: Gather dataset(s) informaton and store in DataFrame\n",
    "    print('Step 1: Gathering dataset(s) informaton and storing in DataFrame...')\n",
    "\n",
    "    data = data.drop(c_to_drop, axis=1)\n",
    "\n",
    "    for c_to_slice,slicer in columns_to_slice:\n",
    "        data['sliced_{}'.format(c_to_slice)] = [str(list(string.split('{}'.format(slicer)))).lower() for string in data[c_to_slice]]\n",
    "\n",
    "\n",
    "    # Step 2: Handeling Missing Values\n",
    "    print('Step 2: Handeling Missing Values...')\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    def handle_missing_values(df):\n",
    "        # create a copy of the dataframe\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # loop through columns in dataframe\n",
    "        for col in df_copy.columns:\n",
    "            # check column data type\n",
    "\n",
    "            try:\n",
    "\n",
    "                if df_copy[col].dtype == object:\n",
    "                    # Handle missing values for categorical columns\n",
    "                    try:\n",
    "                        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                        df_copy[col] = categorical_imputer.fit_transform(df_copy[[col]])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif df_copy[col].dtype == 'int64' or df_copy[col].dtype == 'float64':\n",
    "                    # Handle missing values for numerical columns\n",
    "                    try:\n",
    "                        numeric_imputer = SimpleImputer(strategy='mean')\n",
    "                        df_copy[col] = numeric_imputer.fit_transform(df_copy[[col]])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif df_copy[col].dtype == 'datetime64[ns]':\n",
    "                    # Handle missing values for datetime columns\n",
    "                    try:\n",
    "                        datetime_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                        df_copy[col] = datetime_imputer.fit_transform(df_copy[[col]])\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            return df_copy.reset_index(drop=True)\n",
    "        except: \n",
    "            return df_copy\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    #avoiding internal structure inconsistences\n",
    "    #data =  pd.DataFrame(data.to_dict())\n",
    "    \n",
    "    data = handle_missing_values(data)\n",
    "    # display(data)\n",
    "    \n",
    "    #avoiding internal structure inconsistences\n",
    "    #data =  pd.DataFrame(data.to_dict())\n",
    "\n",
    "\n",
    "    # Step 3: Scale non-numerical data\n",
    "    print('Step 3: Scaling non-numerical data...')\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import ast\n",
    "\n",
    "    #Função para fazer o encoding de uma lista de dicionários dentro de uma linha \n",
    "    def encode_dicts_in_column(df, data_col):\n",
    "\n",
    "        def one_hot_encode_numeric_dicts_in_column(df, data_col):\n",
    "            '''Dado um dataframe com uma coluna de listas de dicionários, identifica automaticamente todas as chaves cujos valores são\n",
    "            numéricos (int ou float), cria uma coluna para cada chave, e o soma s valores de mesma chave.\n",
    "            Se os valores encontrados na chave forem objeto, pula a chave.'''\n",
    "\n",
    "            c_to_drop = list(df.columns)\n",
    "\n",
    "            rows = []\n",
    "            for _, row in df.iterrows():\n",
    "                if isinstance(row[data_col], str):\n",
    "                    list_of_dicts = ast.literal_eval(row[data_col])  #ensure list-like\n",
    "                elif pd.isnull(row[data_col]).all():\n",
    "                    list_of_dicts = []\n",
    "                else:\n",
    "                    list_of_dicts = row[data_col]\n",
    "\n",
    "                row_dict = {}\n",
    "                for d in list_of_dicts:\n",
    "                    for key, value in d.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            row_dict[key] = row_dict.get(key, 0) + value\n",
    "\n",
    "                rows.append(row_dict)\n",
    "\n",
    "            new_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(rows)], axis=1)\n",
    "            new_df = new_df.drop(c_to_drop+[data_col], axis=1)\n",
    "            new_df = new_df.fillna(0)\n",
    "            new_df.reset_index(drop=True, inplace=True)\n",
    "            return new_df\n",
    "\n",
    "\n",
    "        def one_hot_encode_object_dicts_in_column(df, data_col):\n",
    "            \"\"\"\n",
    "            Dado um dataframe com uma coluna de listas de dicionários, identifica automaticamente todas as chaves cujos valores são\n",
    "            objetos (strings), cria uma coluna para cada valor exclusivo encontrado na chave, mas apenas para valores do tipo objeto.\n",
    "            Se os valores encontrados na chave forem numéricos, pula a chave.\n",
    "            \"\"\"\n",
    "            c_to_drop = list(df.columns)\n",
    "            new_df = df.copy()\n",
    "\n",
    "            all_keys = set()\n",
    "            all_objects = set()\n",
    "            for row in new_df[data_col]:\n",
    "                if row is not np.nan:\n",
    "                    if isinstance(row, str):\n",
    "                        row = ast.literal_eval(row) #ensure list-like\n",
    "                    for d in row:\n",
    "                        all_keys |= set(d.keys())\n",
    "                        for k, v in d.items():\n",
    "                            if isinstance(v, str):\n",
    "                                all_objects.add(k)\n",
    "\n",
    "            for obj in all_objects:\n",
    "                # Verifica se a coluna já existe antes de criar uma nova coluna\n",
    "                if obj not in new_df.columns:\n",
    "                    new_df[obj] = 0\n",
    "\n",
    "            for i, row in new_df.iterrows():\n",
    "                log_data = row[data_col]\n",
    "                if log_data is not np.nan:\n",
    "                    if isinstance(log_data, str):\n",
    "                            log_data = ast.literal_eval(log_data) #ensure list-like\n",
    "                    for d in log_data:\n",
    "                        for k, v in d.items():\n",
    "                            if isinstance(v, str) and k in all_objects:\n",
    "                                # Usa o operador de atribuição \"+=\" para adicionar valores a uma coluna existente\n",
    "                                new_df.loc[i, v] = 1\n",
    "\n",
    "            c_to_drop = c_to_drop + list(all_objects)\n",
    "            new_df = new_df.drop(c_to_drop, axis=1)\n",
    "            new_df = new_df.fillna(0)\n",
    "            return new_df\n",
    "\n",
    "\n",
    "        final_df = pd.concat([df, one_hot_encode_numeric_dicts_in_column(df, data_col), one_hot_encode_object_dicts_in_column(df, data_col)], axis=1)\n",
    "        final_df = final_df.drop(data_col, axis=1)\n",
    "        return final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "    def auto_scaler(df, target_col):\n",
    "        # create a copy of the dataframe\n",
    "        df_scaled = df.copy()\n",
    "\n",
    "        for col in [c for c in df_scaled.columns if c!=target_col]:\n",
    "            # check column data type List of Dicts\n",
    "            try:\n",
    "                if str(df_scaled[col][0]).startswith('[{'):\n",
    "                    # master one-hot encoding\n",
    "                    df_scaled = encode_dicts_in_column(df_scaled, col)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        # loop through columns in dataframe\n",
    "        for col in [c for c in df_scaled.columns if c!=target_col]:\n",
    "            # check column data type\n",
    "            try:\n",
    "                #Testando e preparando dados em colunas de listas\n",
    "                if str(df_scaled[col][0])[0]=='[':\n",
    "                    \n",
    "                    try:\n",
    "                        # ensure list-like\n",
    "                        df_scaled[col] = df_scaled[col].apply(lambda x: ast.literal_eval(x))\n",
    "                        # one-hot encoding\n",
    "                        one_hot = pd.get_dummies(df_scaled[col].apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "                        # concatenate the original data and one-hot encoding\n",
    "                        df_scaled = pd.concat([df_scaled, one_hot], axis=1)\n",
    "                        # drop the original 'lists' column\n",
    "                        df_scaled.drop(col, axis=1, inplace=True)\n",
    "                    \n",
    "                    except Exception as err:\n",
    "                        print(err)\n",
    "                \n",
    "                elif df_scaled[col].dtypes == 'object':\n",
    "                    \"\"\"\n",
    "                    Dado um dataframe, tenta converter a coluna de strngs em objetos de data/hora usando a função pd.to_datetime.\n",
    "                    Usa o parâmetro infer_datetime_format=True para tentar detectar automaticamente o formato da data na string.\n",
    "                    Se isso Não for bem sucedido, usa one-hot encoding\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    from dateutil import parser\n",
    "                    \n",
    "                    def is_date_column(series, threshold=0.9):\n",
    "                        \"\"\"\n",
    "                        Verifica se uma coluna é provavelmente uma coluna de datas.\n",
    "\n",
    "                        Parameters:\n",
    "                        - series: pd.Series\n",
    "                            A coluna a ser verificada.\n",
    "                        - threshold: float, opcional (padrão=0.9)\n",
    "                            Limite de confiança para decidir se a coluna é provavelmente uma coluna de datas.\n",
    "\n",
    "                        Returns:\n",
    "                        - bool\n",
    "                            True se a coluna é provavelmente uma coluna de datas, False caso contrário.\n",
    "                        \"\"\"\n",
    "                        try:\n",
    "                            parsed_dates = series.apply(parser.parse)\n",
    "                            valid_dates = parsed_dates[parsed_dates.notnull()]\n",
    "                            date_ratio = len(valid_dates) / len(series)\n",
    "                            return date_ratio >= threshold\n",
    "                        except Exception as e:\n",
    "                            return False\n",
    "                    \n",
    "                    if is_date_column(df_scaled[col]):\n",
    "                        df_scaled[col] = pd.to_datetime(df_scaled[col], infer_datetime_format=True)\n",
    "                    else:\n",
    "                        # Decide se vai numerizar a coluna de objeto usando Get_dummies ou Label_encoder\n",
    "                        \n",
    "                        unique_values = df_scaled[col].nunique()\n",
    "                        total_rows = len(df_scaled)\n",
    "\n",
    "                        threshold = 0.1 #Label encoder só se até a coluna ter 10% de valores únicos\n",
    "                        \n",
    "                        if unique_values / total_rows < threshold:\n",
    "                            # Usar LabelEncoder se a proporção de valores distintos for menor que o limiar\n",
    "                            label_encoder = LabelEncoder()\n",
    "                            \n",
    "                            # Ajustar e transformar a coluna de strings\n",
    "                            df_scaled[col] = label_encoder.fit_transform(df_scaled[col])\n",
    "\n",
    "                            # Criar um dicionário de mapeamento entre valores originais e valores numéricos\n",
    "                            mapeamento = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "                            # Exibir o dicionário de mapeamento\n",
    "                            maps.append({col:mapeamento})\n",
    "                            \n",
    "                        else:\n",
    "                            # Usar get_dummies se a proporção de valores distintos for maior ou igual ao limiar\n",
    "                            df_scaled = pd.get_dummies(df_scaled, columns=[col], prefix=[col])\n",
    "\n",
    "\n",
    "                elif df_scaled[col].dtypes == 'int64' or df_scaled[col].dtypes == 'float64':\n",
    "                    # use MinMaxScaler for numerical variables\n",
    "                    scaler = MinMaxScaler()\n",
    "                    # df_scaled[[col]] = scaler.fit_transform(df_scaled[[col]])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #Fazendo o tratamento da coluna-alvo para Modelos de Agrupamento\n",
    "        if df_scaled[target_col].dtypes == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df_scaled[target_col] = le.fit_transform(df_scaled[target_col])\n",
    "        \n",
    "        try:\n",
    "            return df_scaled.reset_index(drop=True)\n",
    "        except:\n",
    "            return df_scaled\n",
    "\n",
    "    \n",
    "    #avoiding internal structure inconsistences\n",
    "    #data =  pd.DataFrame(data.to_dict())\n",
    "    \n",
    "    data = auto_scaler(data, target_column_name)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\\n\\nDados tratados em {:.2f} minutos\".format((end-start)/60))\n",
    "\n",
    "    return data, maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa95a5",
   "metadata": {},
   "source": [
    "# Ranking V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5da3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta para descobrir todas as Ofertas coletadas pelo Scrapping da Carhunt\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "client = MongoClient('mongodb+srv://gabrielaraujon96:D3umaten0ve@carhunt.sweobl3.mongodb.net/')\n",
    "db = client['carhunt']\n",
    "\n",
    "#dados de ofertas:\n",
    "base_v = pd.DataFrame(db['search_scraping'].find())\n",
    "\n",
    "# Drop all rows that contain any NaN values\n",
    "base_v = base_v.dropna(subset=['brand', 'city', 'km', 'model',\n",
    "       'model_year', 'price', 'state'])\n",
    "\n",
    "\n",
    "display(base_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d784ea6",
   "metadata": {},
   "source": [
    "## Pré engenharia de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Clone da base original\n",
    "coerced_base_v = base_v.copy()\n",
    "\n",
    "#Preparação dos dados das ofertas para ML\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Colunas que PRECISAM ser numéricas\n",
    "num_cols = ['km', 'model_year', 'price']\n",
    "\n",
    "# casting das colunas que devem ser numéricas por natureza\n",
    "for c in num_cols:\n",
    "    coerced_base_v[c] = [float(x) for x in coerced_base_v[c]]\n",
    "\n",
    "# Criando coluna de age (anos desde a fabricação) que diminui a dimensionalidade dos dados\n",
    "coerced_base_v['age'] = datetime.datetime.now().year-coerced_base_v['model_year'].astype(int)\n",
    "\n",
    "# Dividir a coluna 'model' em duas colunas: 'model' e 'type'\n",
    "coerced_base_v[['model', 'type']] = coerced_base_v['model'].str.split(pat=' ', n=1, expand=True)\n",
    "\n",
    "# Adicionar uma nova coluna condicional\n",
    "coerced_base_v['transmission'] = coerced_base_v['type'].apply(lambda x: 'Aut.' if 'aut' in x.lower() else 'Manual')\n",
    "\n",
    "# Reajustar coluna de Type, removendo transmissão\n",
    "coerced_base_v['type'] = coerced_base_v['type'].apply(lambda x: ' '.join([word for word in x.split() if(('aut' not in word.lower()) and ('manu' not in word.lower()))]))\n",
    "\n",
    "# # Tipos de carrocerias de carros, para categorizar\n",
    "# # https://www.icarros.com.br/catalogo/compare.jsp\n",
    "# tipos_normalizados = ['Hatch (compacto)','Sedan','Picape','SUV (Utilitário esportivo)','Monovolume','Perua/Wagon (SW)','Van','Conversível (coupé)','Hibrido/Elétrico']\n",
    "\n",
    "# norm_body_types = { 'Conversível': 'Conversível (coupé)',\n",
    "#   'Coupé': 'Conversível (coupé)',\n",
    "#   'Conversível (coupé)':'Conversível (coupé)',\n",
    "#   'Coupê': 'Conversível (coupé)',\n",
    "#   'Cupê': 'Conversível (coupé)',\n",
    "#   'Hatch': 'Hatch (compacto)',\n",
    "#   'Hatchback': 'Hatch (compacto)',\n",
    "#   'Hatch (compacto)':'Hatch (compacto)',\n",
    "#   'Compacto': 'Hatch (compacto)',\n",
    "#   'Minivan': 'Van',\n",
    "#   'Van/Utilitário': 'Van',\n",
    "#   'Van':'Van',\n",
    "#   'Furgão & Van': 'Van',\n",
    "#   'Monovolume': 'Monovolume',\n",
    "#   'Perua/SW': 'Perua/Wagon (SW)',\n",
    "#   'SW & Perua': 'Perua/Wagon (SW)',\n",
    "#   'Perua/Wagon (SW)':'Perua/Wagon (SW)',\n",
    "#   'Picape': 'Picape',\n",
    "#   'Picapes': 'Picape',\n",
    "#   'Sedan': 'Sedan',\n",
    "#   'Sedã': 'Sedan',\n",
    "#   'Utilitário esportivo': 'SUV (Utilitário esportivo)',\n",
    "#   'SUV (Utilitário esportivo)': 'SUV (Utilitário esportivo)',\n",
    "#   'SUV': 'SUV (Utilitário esportivo)',\n",
    "#   'SUV/Utilitário': 'SUV (Utilitário esportivo)',\n",
    "#   'Hibrido':'Hibrido/Elétrico',\n",
    "#   'Elétrico':'Hibrido/Elétrico',\n",
    "#   'Hibrido/Elétrico':'Hibrido/Elétrico',\n",
    "#   }\n",
    "\n",
    "# # Fazendo a tradução na base:\n",
    "# coerced_base_v['body'] = coerced_base_v['body'].map(norm_body_types)\n",
    "\n",
    "#Setando todos os Body Types como SUv (ATE QUE O GABRIEL ADICIONE A COLUNA BODY)\n",
    "coerced_base_v[\"body\"] = 'SUV'\n",
    "\n",
    "display(coerced_base_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bdb18",
   "metadata": {},
   "source": [
    "# Preparação dos dados das ofertas para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431904c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Transformada para ML\n",
    "\n",
    "#ARMAZENA A COLUNA ID ANTES DE DROPAR\n",
    "id_column = coerced_base_v[['_id']].copy()\n",
    "id_column['original_index'] = coerced_base_v.index\n",
    "results_rank_df = coerced_base_v.drop(columns=['_id'])\n",
    "\n",
    "\n",
    "#results_rank_df = coerced_base_v.drop(columns=['_id'])\n",
    "\n",
    "#Colunas irrelevantes para a aprendizaem no que tange precificação\n",
    "irrelevant_columns = ['_id', 'model_year', 'fab_year', 'link', 'source', 'ad_id', 'image', 'type']\n",
    "\n",
    "target_col = \"price\"\n",
    "\n",
    "base_v_toML, base_v_maps = prepare_data_for_ML(coerced_base_v,target_col,irrelevant_columns,[]) #[('type', ' ')]\n",
    "\n",
    "# Aqui descobrimos quais colunas tiveram seus dados categóricos encoded como numericos\n",
    "# pois para fins de exibição ao final das análises é necessário retraduzir\n",
    "base_v_colunas_mapeadas = [list(m.keys())[0] for m in base_v_maps]\n",
    "print('colunas mapeadas com Label Encoding:',base_v_colunas_mapeadas)\n",
    "display(base_v_toML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080b16",
   "metadata": {},
   "source": [
    "# Detecção de Anomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "import sklearn as sk\n",
    "# print(sk.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "def detect_outliers(df, contamination=0.05, random_state=42, n_jobs=-1):\n",
    "    \n",
    "    s = time.time()\n",
    "    \n",
    "    # Create a copy of the input data frame\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # Store column names\n",
    "    columns = df_scaled.columns\n",
    "\n",
    "    # Perform Isolation Forest with hyperparameter tuning\n",
    "    isolation_forest_params = {'n_estimators': [50, 100, 200], 'max_samples': ['auto', 0.5, 0.7], 'contamination': [contamination]}\n",
    "    isolation_forest = GridSearchCV(IsolationForest(random_state=random_state), isolation_forest_params, scoring='neg_mean_squared_error', cv=3, n_jobs=n_jobs)\n",
    "    isolation_forest.fit(df_scaled)\n",
    "    df_scaled['isolation_forest'] = isolation_forest.predict(df_scaled)\n",
    "\n",
    "    # Perform One-Class SVM with hyperparameter tuning\n",
    "    svm_params = {'kernel': ['linear', 'rbf'], 'nu': [0.01, 0.05, 0.1]}\n",
    "    one_class_svm = GridSearchCV(OneClassSVM(), svm_params, scoring='neg_mean_squared_error', cv=3, n_jobs=n_jobs)\n",
    "    one_class_svm.fit(df_scaled)\n",
    "    df_scaled['one_class_svm'] = one_class_svm.predict(df_scaled)\n",
    "\n",
    "    # Perform Local Outlier Factor\n",
    "    lof = LocalOutlierFactor(contamination=contamination)\n",
    "    df_scaled['lof'] = lof.fit_predict(df_scaled)\n",
    "    \n",
    "    # Count the number of '-1' values in each row\n",
    "    outlier_counts = df_scaled[['isolation_forest', 'one_class_svm', 'lof']].eq(-1).sum(axis=1)\n",
    "    \n",
    "    # Remove classification columns\n",
    "    df_scaled = df_scaled.drop(columns=['isolation_forest', 'one_class_svm', 'lof'])\n",
    "    \n",
    "    # Identify rows with more than one classification as an outlier\n",
    "    outlier_rows = df_scaled[outlier_counts > 1]\n",
    "    outlier_rows['outlier_flag'] = 1\n",
    "\n",
    "    # Keep only rows that are not classified as outliers by any model\n",
    "    df_cleaned = df_scaled.drop(outlier_rows.index, axis=0)\n",
    "    df_cleaned['outlier_flag'] = 0\n",
    "\n",
    "    # Evaluate performance\n",
    "    print('Total time for outlier detection: {:,.2f} seconds'.format(time.time()-s))\n",
    "    print('{:,.2f}% of the base were outliers ({:,})'.format((len(outlier_rows)/len(df_cleaned))*100, len(outlier_rows)))\n",
    "\n",
    "    return pd.concat([df_cleaned, outlier_rows]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddcae29",
   "metadata": {},
   "source": [
    "## Modelos de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600654f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testador de modelos de regressão\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor, OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, ARDRegression, PassiveAggressiveRegressor, TheilSenRegressor, SGDRegressor, Lars, LassoLars, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor#, RandomSubspace\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "def select_best_regression_model(data, target_col, cv_strategy='default', n_jobs=1, cv=None):\n",
    "    \n",
    "    start_global = time.time()\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    # Regression Models\n",
    "    regression_models = [\n",
    "        LinearRegression(),\n",
    "        Ridge(alpha=1.0),\n",
    "        Lasso(alpha=1.0),\n",
    "        ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "        BayesianRidge(),\n",
    "        HuberRegressor(),\n",
    "        GaussianProcessRegressor(),\n",
    "#         RandomSubspace(),  # Ensemble method\n",
    "        StackingRegressor(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestRegressor()),\n",
    "                ('gb', GradientBoostingRegressor())\n",
    "            ],\n",
    "            final_estimator=LinearRegression()\n",
    "        ),\n",
    "        OrthogonalMatchingPursuit(),\n",
    "        OrthogonalMatchingPursuitCV(),\n",
    "        ARDRegression(),\n",
    "        PassiveAggressiveRegressor(),\n",
    "        TheilSenRegressor(),\n",
    "        SVR(),\n",
    "        DecisionTreeRegressor(),\n",
    "        RandomForestRegressor(),\n",
    "        GradientBoostingRegressor(n_iter_no_change=10, tol=0.01),  # Early stopping\n",
    "        MLPRegressor(max_iter=1000, early_stopping=True, n_iter_no_change=10),  # Early stopping\n",
    "        SGDRegressor(),\n",
    "        Lars(),\n",
    "        LassoLars(),\n",
    "        LassoLarsCV(),\n",
    "        LassoLarsIC()\n",
    "    ]\n",
    "\n",
    "    # Create KFold cross-validation object\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model in regression_models:\n",
    "        model_name = model.__class__.__name__\n",
    "        print('Testing model: {}'.format(model_name))\n",
    "\n",
    "        try:\n",
    "            if model_name in ['PCA', 'KernelPCA', 'TSNE']:\n",
    "                model.fit(data.drop(target_col, axis=1))\n",
    "                transformed_data = model.transform(data.drop(target_col, axis=1))\n",
    "                reg_model = LinearRegression().fit(transformed_data, data[target_col])\n",
    "                predictions = reg_model.predict(transformed_data)\n",
    "            else:\n",
    "                if model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "                    # Regularization for specific models\n",
    "                    param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "                    model = HalvingGridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_squared_error', n_jobs=n_jobs)\n",
    "\n",
    "                # Hyperparameter Tuning\n",
    "                if cv_strategy == 'grid_search':\n",
    "                    param_grid = {}  # Add hyperparameters for tuning\n",
    "                    model = HalvingGridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_squared_error', n_jobs=n_jobs)\n",
    "\n",
    "                if hasattr(model, 'n_iter_no_change') and hasattr(model, 'tol'):\n",
    "                    # Early Stopping for models that support it\n",
    "                    model.n_iter_no_change = 10  # Number of consecutive iterations with no improvement\n",
    "                    model.tol = 0.01  # Tolerance to declare convergence\n",
    "\n",
    "                model.fit(data.drop(target_col, axis=1), data[target_col])\n",
    "                predictions = model.predict(data.drop(target_col, axis=1))\n",
    "\n",
    "            # Regression Model Scoring\n",
    "            if cv_strategy != 'grid_search':\n",
    "                # Cross-Validation Strategy\n",
    "                if cv_strategy == 'stratified_kfold':\n",
    "                    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                else:\n",
    "                    cv_scores = cross_val_score(model, data.drop(target_col, axis=1), data[target_col], cv=kf, scoring='neg_mean_squared_error')\n",
    "                    cv_score = cv_scores.mean()\n",
    "            else:\n",
    "                cv_score = None\n",
    "\n",
    "            # Regression scores\n",
    "            r2 = r2_score(data[target_col], predictions)\n",
    "            mse = mean_squared_error(data[target_col], predictions)\n",
    "            mae = mean_absolute_error(data[target_col], predictions)\n",
    "\n",
    "            scores[model_name] = {'trained_model': model, 'r2_score': r2, 'mse_score': mse, 'mae_score': mae, 'cv_score': cv_score}\n",
    "\n",
    "        except Exception as erro:\n",
    "            print(\"Deu ruim: {}\".format(erro))\n",
    "\n",
    "            \n",
    "    # -----------------------------------------------\n",
    "    # Selecting the best model based on cost-benefit score\n",
    "    scores_df = pd.DataFrame(scores).T.reset_index().rename(columns={'index':'model'})\n",
    "    \n",
    "    scores_df['final_score'] =  scores_df['mse_score'] / scores_df['r2_score']\n",
    "    \n",
    "    positive_scores_df = scores_df[scores_df['r2_score']>0].sort_values(by='final_score', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    best_model = list(positive_scores_df['trained_model'])[0]\n",
    "            \n",
    "    # Performance Visualization for the best model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=data[target_col], y=best_model.predict(data.drop(target_col, axis=1)))\n",
    "    plt.title(f\"True vs. Predicted Values - {best_model.__class__.__name__}\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.show()\n",
    "\n",
    "    # Learning Curve for the best model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_learning_curve(best_model, best_model.__class__.__name__, data.drop(target_col, axis=1), data[target_col], cv=cv)\n",
    "    plt.show()\n",
    "\n",
    "    # Model Persistence\n",
    "    # Save the best model to disk for later use\n",
    "    #joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "    \n",
    "    end_global = time.time()\n",
    "    print('\\nThe best Regression model is:', list(positive_scores_df['model'])[0])\n",
    "    \n",
    "    print(\"\\nEstudo finalzado em {:.2f} segundos\".format((end_global - start_global)))\n",
    "    \n",
    "    return best_model, scores_df\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Learning Curve - {title}\")\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# target_col = 'sre_value'\n",
    "# best_model, scores_df = select_best_regression_model(data, target_col, cv_strategy='grid_search', n_jobs=-1, cv=None)\n",
    "# display(scores_df)\n",
    "# best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização de modelo\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "#import networkx as nx\n",
    "import time\n",
    "\n",
    "def visualize_best_model(best_model, data, target_col):\n",
    "    start_global = time.time()\n",
    "    \n",
    "    if 'HalvingGridSearchCV' in str(best_model.__class__):\n",
    "        # Extract the best estimator from HalvingGridSearchCV\n",
    "        best_estimator = best_model.best_estimator_\n",
    "        visualize_estimator(best_estimator, data, target_col)\n",
    "    else:\n",
    "        visualize_estimator(best_model, data, target_col)\n",
    "        \n",
    "    end_global = time.time()    \n",
    "    print(\"\\nPlot finalzado em {:.2f} min\".format((end_global - start_global)/60))\n",
    "\n",
    "    \n",
    "def visualize_estimator(estimator, data, target_col):\n",
    "    model_type = estimator.__class__.__name__\n",
    "\n",
    "    if model_type == 'DecisionTreeRegressor':\n",
    "        # Visualize decision tree and print number of nodes and edges\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(estimator, filled=True, feature_names=data.drop(target_col, axis=1).columns)\n",
    "        plt.show()\n",
    "\n",
    "        num_nodes = estimator.tree_.node_count\n",
    "        num_edges = num_nodes - 1\n",
    "        print(f\"Number of nodes: {num_nodes}, Number of edges: {num_edges}\")\n",
    "\n",
    "    elif model_type == 'MLPRegressor':\n",
    "        # Visualize neural network architecture (simplified for illustration)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Neural Network Architecture\")\n",
    "        plt.imshow([len(data.drop(target_col, axis=1).columns), 10, 1], cmap='viridis', aspect='auto', extent=(0, 1, 0, 1))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        num_neurons = sum(layer_size for layer_size in estimator.hidden_layer_sizes)\n",
    "        print(f\"Number of neurons: {num_neurons}\")\n",
    "\n",
    "    elif model_type in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "        # Visualize linear regression coefficients or other relevant information\n",
    "        if hasattr(estimator, 'coef_'):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(data.drop(target_col, axis=1).columns, estimator.coef_)\n",
    "            plt.title(\"Linear Regression Coefficients\")\n",
    "            plt.xlabel(\"Feature\")\n",
    "            plt.ylabel(\"Coefficient Value\")\n",
    "            plt.show()\n",
    "\n",
    "            num_coefficients = len(estimator.coef_)\n",
    "            print(f\"Number of coefficients: {num_coefficients}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Visualization not implemented for {model_type}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# visualize_best_model(best_model, data, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ddcc6",
   "metadata": {},
   "source": [
    "# Código de Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88146fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Código de Ranking\n",
    "#By DataMaster\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# recebe um dataframe com abslutamente todas as ofertas da carhunt\n",
    "def ML_ranking_gen2(df, df_maps, train_method='arbit', target_col = 'price', split_col = 'body'):\n",
    "    \n",
    "    #Clone dos dados de pesquisa, para não alterar os originais\n",
    "    all_data = df.copy()\n",
    "    \n",
    "    start_global = time.time()\n",
    "\n",
    "    # COnversão obrigatória dos valores dos carros\n",
    "    all_data[target_col] = pd.to_numeric(all_data[target_col], errors=\"coerce\")\n",
    "        \n",
    "    # Engenharia de recursos ----\n",
    "    # usando Information gain para performar a divisão de categorias\n",
    "    # ------\n",
    "    \n",
    "    # aqui vamos arbitrariamente usar 'sre_body_type' como divisão inicial da base\n",
    "    categories = list(dict.fromkeys(all_data[split_col]))\n",
    "\n",
    "    # Criação de um dicionário com modelos de machine learning treinados\n",
    "    CarHuntML_models = {}\n",
    "    \n",
    "    # listapara salvar todos os chunks de dados separados no treiamneot dos modelos\n",
    "    data_slices = []\n",
    "    \n",
    "    for cat in categories:\n",
    "        \n",
    "        data = all_data[all_data[split_col]==cat]\n",
    "        \n",
    "        # Se eu tenho dados:\n",
    "        if len(data)>6:\n",
    "            \n",
    "            # qual o index em que a split_col está no mapa de dados\n",
    "\n",
    "            splt_col_maps_idx = next((i for i, d in enumerate(df_maps) if list(d.keys())[0] == split_col), None)\n",
    "            \n",
    "            str_body_type = [k for k,v in df_maps[splt_col_maps_idx][split_col].items() if v==cat][0]\n",
    "            print('\\n{}'.format(str_body_type))\n",
    "\n",
    "            original_data = all_data[all_data[split_col]==cat]\n",
    "\n",
    "            # Se alguma das colunas passou reto na preparação para ML, remove aqui mesmo\n",
    "            data = original_data.select_dtypes(include='number')\n",
    "\n",
    "            print('Detecting outliers...')\n",
    "            # Aqui é feita uma detecção de outliers na categoria\n",
    "            data = detect_outliers(data)\n",
    "\n",
    "\n",
    "            print('Training model...')\n",
    "\n",
    "            # Create KFold cross-validation object\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            if train_method=='auto':\n",
    "                # testes de modelos diferentes de Machine Learning para previsão do Preço (regressão)\n",
    "                best_model, scores_df = select_best_regression_model(data, target_col, cv_strategy='grid_search', n_jobs=-1, cv=kf)\n",
    "\n",
    "\n",
    "            elif train_method=='arbit':\n",
    "                # Arbitratiamente setando o melhor modelo como DecisionTreeRegressor()\n",
    "\n",
    "                # For HalvingGridSearchCV \n",
    "                scoring_m = 'neg_mean_squared_error' #'r2'\n",
    "\n",
    "                if len(data)*len(data.columns)<1000000: #(menor que 1M amostras)\n",
    "                    # sem evitar overfitting, pois o conjunto de dados é \"pequeno\" para ML \n",
    "                    print('Overfitting pois temos poucos dados')\n",
    "\n",
    "                    try:\n",
    "                        best_model = HalvingGridSearchCV(estimator=DecisionTreeRegressor(), n_jobs=-1, param_grid={}, scoring=scoring_m, cv=kf)\n",
    "                        best_model.fit(data.drop(target_col, axis=1), data[target_col])\n",
    "                    except:\n",
    "                        pass\n",
    "                else: #(maior que 1M amostras)\n",
    "                    # Evitando overfitting, pois o conjunto de dados é grande o suficiente para maiores generalizações\n",
    "                    print('Generalizando pois temos muitos dados')\n",
    "\n",
    "                    # Tunando hiperparâmetros do modelo\n",
    "                    # Automatically determine reasonable ranges for hyperparameters based on data size\n",
    "                    max_depth_range = [None] + list(range(5, int((len(data.columns)-1)**1.75), int(len(data)/100)))\n",
    "                    min_samples_split_range = list(range(2, int(len(data)**0.5), int(len(data)/100)))\n",
    "                    min_samples_leaf_range = list(range(int(len(data)**0.15), int(len(data)**0.75), int(len(data)/100)))\n",
    "\n",
    "                    # Define the parameter grid for the decision tree\n",
    "                    param_grid = {\n",
    "                        'max_depth': max_depth_range,\n",
    "                        'min_samples_split': min_samples_split_range,\n",
    "                        'min_samples_leaf': min_samples_leaf_range}\n",
    "\n",
    "                    # Create the DecisionTreeRegressor and HalvingGridSearchCV objects\n",
    "                    regressor = DecisionTreeRegressor()\n",
    "                    grid_search = HalvingGridSearchCV(\n",
    "                        estimator=regressor,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring=scoring_m,\n",
    "                        n_jobs=-1,\n",
    "                        cv=kf)\n",
    "\n",
    "                    # Fit the model to the data\n",
    "                    grid_search.fit(data.drop(target_col, axis=1), data[target_col])\n",
    "\n",
    "                    # Get the best model\n",
    "                    best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "            # Salvando o modelo no dicionário, com o tipo de carroceria treinada\n",
    "            CarHuntML_models[str_body_type] = best_model\n",
    "\n",
    "            # Visualização\n",
    "            #display(scores_df)    \n",
    "            #visualize_best_model(best_model, data, target_col)\n",
    "\n",
    "            # RANKING\n",
    "            # Aqui vamos medir a distânica entre o valor previsto pelo modelo de precificação e o valor real\n",
    "            # Se a diferença (valor_previsto - valor_real) for positiva, que dizer que o carro está com um desconto de diff\n",
    "            # Agora se a diferença for negativa, que dizer que o carro está com um preço de diff acima do esperado\n",
    "\n",
    "            # Desta maneira a maior diff será d melhor carro desa categoria, afinal está com o maior desconto\n",
    "\n",
    "            predicted_values = list(best_model.predict(data.drop(target_col, axis=1)))\n",
    "\n",
    "            data['diff_ml'] = [valor_previsto - valor_real  for valor_previsto, valor_real  in \n",
    "                            zip(predicted_values, list(data[target_col]))]\n",
    "\n",
    "            # Em seguida vamos fazer uma escala de 0 a 10 com os valores de diff, \n",
    "            # e assim determinar notas para o RANKING CARHUNT\n",
    "\n",
    "            # Create a MinMaxScaler instance and fit_transform the data\n",
    "            scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "            # Reshape the data as MinMaxScaler expects a 2D array\n",
    "            # Then Extract the normalized values from the resulting 2D array\n",
    "            data['Ranking_CarHunt'] = [v[0] for v in scaler.fit_transform([[x/pv] for x,pv in zip(data['diff_ml'], predicted_values)])]\n",
    "\n",
    "            data['predicted_ML_value'] = predicted_values\n",
    "\n",
    "            print('Model trained!')\n",
    "\n",
    "            # Readicionando as colunas não numéricas, se necessário\n",
    "            inverse_selection = original_data.select_dtypes(exclude='number')\n",
    "\n",
    "            # Concatenate the two DataFrames along the columns axis (axis=1)\n",
    "            data = pd.concat([data, inverse_selection], axis=1).sort_index()\n",
    "\n",
    "            #display(data)\n",
    "\n",
    "            data_slices.append(data)\n",
    "    \n",
    "    \n",
    "    # Reunificando dados -----\n",
    "    all_data = pd.concat(data_slices).sort_index()\n",
    "\n",
    "    end_global = time.time()\n",
    "    print(\"Ranking finalizado em {:.2f} min\".format((end_global - start_global)/60))\n",
    "    \n",
    "    return all_data, CarHuntML_models\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# CHAMADA\n",
    "\n",
    "results_rank_df, CarHuntML_models = ML_ranking_gen2(base_v_toML, df_maps=base_v_maps, train_method='arbit', target_col = 'price', split_col = 'body')\n",
    "\n",
    "# Model Persistence\n",
    "# Save the best models to disk for later use\n",
    "joblib.dump(CarHuntML_models, 'CarHuntML_models_{}.pkl'.format(datetime.datetime.now().strftime(\"%d%b%Y-%Hh%M\")))    \n",
    "\n",
    "display(results_rank_df)\n",
    "\n",
    "\n",
    "#Retraduzindo os dados mapeados:\n",
    "for col in [c for c in base_v_colunas_mapeadas if c in list(results_rank_df.columns)]:\n",
    "\n",
    "    # encontrando o dicionário de mapeamento \n",
    "    mapping_dict = next(item.get(col) for item in base_v_maps if col in item)\n",
    "\n",
    "    results_rank_df[col] = results_rank_df[col].map({v: k for k, v in mapping_dict.items()})\n",
    "    \n",
    "\n",
    "# ReCriando coluna de ano\n",
    "results_rank_df['model_year'] = datetime.datetime.now().year-results_rank_df['age']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a423ede",
   "metadata": {},
   "source": [
    "# RECUPERAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20971468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#==================================================\n",
    "# COMO RECUPERAR AS COLUNAS ANTIGAS SEM EMBARALHAR? (LAÍS)\n",
    "\n",
    "\n",
    "results_rank_df['original_index'] = results_rank_df.index\n",
    "\n",
    "#renomea temporariamente a coluna '_id' p/ evitar conflitos \n",
    "results_rank_df['original_index'] = results_rank_df.index\n",
    "\n",
    "id_column = id_column.rename(columns={'_id': '_id_temp'})\n",
    "\n",
    "# merge com base no indice original\n",
    "results_rank_df = pd.merge(results_rank_df, id_column, on='original_index')\n",
    "\n",
    "#restaura o nome original da coluna '_id'\n",
    "results_rank_df = results_rank_df.rename(columns={'_id_temp': '_id'})\n",
    "\n",
    "#remove a coluna de indice orig usada para o merge\n",
    "results_rank_df = results_rank_df.drop(columns=['original_index'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#base_v_toML['_id'] = id_column\n",
    "\n",
    "#Restaurar a coluna  de acordo com a ordem correta\n",
    "#results_rank_df['_id'] = id_column\n",
    "\n",
    "#results_rank_df = results_rank_df[['_id'] + [col for col in results_rank_df.columns if col != '_id']]\n",
    "\n",
    "\n",
    "#====================================================\n",
    "\n",
    "# Recuperando o ID original\n",
    "#results_rank_df['_id'] = coerced_base_v['_id']\n",
    "\n",
    "# Recolocando colunas anteriormente ignoradas na tratativa de ML (tentei com pandas merge)\n",
    "final_full_sre_ranking = pd.merge(results_rank_df, coerced_base_v[irrelevant_columns], on='_id', how='inner')\n",
    "\n",
    "\n",
    "#====================================================\n",
    "\n",
    "\n",
    "\n",
    "# Ordenando campeões -----\n",
    "final_full_sre_ranking = final_full_sre_ranking.sort_values(\"Ranking_CarHunt\", ascending=False)\n",
    "\n",
    "# Reorganizando as colunas\n",
    "ml_cols=['outlier_flag','predicted_ML_value','price','diff_ml','Ranking_CarHunt']\n",
    "final_full_sre_ranking = final_full_sre_ranking[[c for c in list(final_full_sre_ranking.columns) if c not in ml_cols]+ml_cols]\n",
    "\n",
    "display(final_full_sre_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_full_sre_ranking.values[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3102d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = ('sre_body_type','Hatch (compacto)')\n",
    "f = ('sre_model', 'hb20')\n",
    "threshold = 9\n",
    "\n",
    "filt_fin_df = final_full_sre_ranking[final_full_sre_ranking[f[0]] == f[1]]\n",
    "# filt_fin_df = filt_fin_df[filt_fin_df['sre_year'] == 2019]\n",
    "\n",
    "top = filt_fin_df[filt_fin_df['Ranking_CarHunt'] > threshold]['sre_value'].mean()\n",
    "general = filt_fin_df[filt_fin_df['Ranking_CarHunt'] <= threshold]['sre_value'].mean()\n",
    "\n",
    "print('Tipo: {}'.format(f[1]))\n",
    "print('Ofertas Analisadas: {:,}'.format(len(filt_fin_df)))\n",
    "\n",
    "print('Preço médio (nota CarHunt acima de {}): R$ {:,.2f}'.format(threshold, top))\n",
    "print('Preço médio (nota CarHunt abaixo de {}): R$ {:,.2f}'.format(threshold, general))\n",
    "\n",
    "print('Diferença R$ {:,.2f}'.format(general-top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac99e2",
   "metadata": {},
   "source": [
    "## Exibição dos Melhores Avaliados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8470e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Testando para ver se a concatenação manteve os indextes, mantedo assim o cruzamento correto de dados\n",
    "# sreid = 230246\n",
    "# display(df_ALL_sre[df_ALL_sre['sre_id']==sreid])\n",
    "# display(final_full_sre_ranking[final_full_sre_ranking['sre_id']==sreid])\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# Filtrando o Olimpo: Carros com o Ranking CarHunt superior a nota 8, sem serem outliers  \n",
    "\n",
    "olimpo_CarHunt = final_full_sre_ranking[(final_full_sre_ranking['Ranking_CarHunt']>9) & (final_full_sre_ranking['outlier_flag']==0)]\n",
    "\n",
    "for bt in list(dict.fromkeys(olimpo_CarHunt['sre_body_type'])):\n",
    "    \n",
    "    print(bt,':')\n",
    "\n",
    "    display(olimpo_CarHunt[olimpo_CarHunt['sre_body_type']==bt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83771fe4",
   "metadata": {},
   "source": [
    "## Visualização do modelo de precificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c5478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = 'Monovolume'\n",
    "\n",
    "target_col = 'sre_value'\n",
    "\n",
    "visualize_best_model(CarHuntML_models[cat], results_rank_df[results_rank_df['sre_body_type']==cat], target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56fa1e",
   "metadata": {},
   "source": [
    "# COMPARAÇÃO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff555e97",
   "metadata": {},
   "source": [
    "### Clsuterizando carros similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clsuterizando carros similares\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "os.environ['OMP_NUM_THREADS'] = str(multiprocessing.cpu_count())\n",
    "\n",
    "\n",
    "#Função auxiliar para determinar o núemro ideal de clsuters \n",
    "def elbow_method(data):\n",
    "    \n",
    "    #realizando a escala dos dados\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    min_clusters = 2\n",
    "    \n",
    "    inercia = []\n",
    "    max_clusters = min_clusters + int(min(data_scaled.shape[0], data_scaled.shape[1])/1.5)\n",
    "    range_clusters = range(min_clusters, max_clusters + 1)\n",
    "    \n",
    "    for k in range_clusters:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "        kmeans.fit(data_scaled)\n",
    "        inercia.append(kmeans.inertia_)\n",
    "\n",
    "        \n",
    "    def encontrar_ponto_mais_agudo(x_range, y_values):\n",
    "        x = np.array(x_range)\n",
    "        y = np.array(y_values)\n",
    "\n",
    "        diff_y = np.diff(y)\n",
    "        diff_x = np.diff(x)\n",
    "\n",
    "        tangentes = diff_y / diff_x\n",
    "\n",
    "        indice_ponto_agudo = np.argmin(tangentes) + 1\n",
    "\n",
    "        return x[indice_ponto_agudo]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def inflexao_segunda_derivada(x,y):\n",
    "        # Calcular a segunda derivada\n",
    "        second_derivative = np.gradient(np.gradient(y, x), x)\n",
    "\n",
    "        # Encontrar o índice do ponto mais agudo\n",
    "        index_max_second_derivative = np.argmax(second_derivative)\n",
    "\n",
    "        # Recuperar as coordenadas do ponto mais agudo\n",
    "        x_max_second_derivative = x[index_max_second_derivative]\n",
    "        y_max_second_derivative = y[index_max_second_derivative]\n",
    "        \n",
    "        return x_max_second_derivative\n",
    "    \n",
    "            \n",
    "        \n",
    "    num_clusters = int((encontrar_ponto_mais_agudo(range_clusters, inercia) + inflexao_segunda_derivada(range_clusters, inercia))/2)\n",
    "    \n",
    "    #contando exceções de regras (colunas em que quase todas as linhas caíram, exceto até 10% das linhas)\n",
    "    #Elas são clsuters novos\n",
    "    count = ((data == 0).sum(axis=0) <= int(len(data)*0.1)).sum()\n",
    "    num_clusters += count\n",
    "    \n",
    "    \n",
    "    fig = px.line(x=range_clusters, y=inercia, title='Elbow')\n",
    "    fig.add_trace(go.Scatter(x=[num_clusters], y=[inercia[num_clusters - min_clusters]], mode='markers', marker=dict(color='red'), name='Optimal K'))\n",
    "    fig.update_xaxes(title='Número de Clusters')\n",
    "    fig.update_yaxes(title='Inércia')\n",
    "    pio.show(fig)\n",
    "\n",
    "    print('Número ideal de clusters calculado: {} Clusters'.format(num_clusters))\n",
    "\n",
    "    return num_clusters\n",
    "\n",
    "\n",
    "\n",
    "def clusterize(data):\n",
    "    \n",
    "    #replicando os dados orginais para não executar alterações indevidas por acidente\n",
    "    dataframe = data.copy()\n",
    "    \n",
    "    #eliminando todas as linhas em que todos os valores são zero\n",
    "    dataframe = (dataframe.loc[~(dataframe.select_dtypes(include='number') == 0).all(axis=1)]).reset_index(drop=True)\n",
    "    \n",
    "    # Determinar o número mínimo de clusters utilizando o método do cotovelo\n",
    "    num_clusters = elbow_method(dataframe.select_dtypes(include='number'))\n",
    "        \n",
    "    # Realizar clusterização com K-means\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(dataframe.select_dtypes(include='number'))\n",
    "\n",
    "    # Adicionar rótulos de cluster como coluna no dataframe original\n",
    "    dataframe['cluster'] = kmeans.labels_\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# CHAMADA DA CLSUTERIZAÇÃO\n",
    "# --------------------------------------------\n",
    "\n",
    "clustered_base_v_toML = clusterize(detc_base_v_toML)\n",
    "\n",
    "clustered_base_v_toML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a2dfa",
   "metadata": {},
   "source": [
    "### PREÇO Calssificação de Ofertas\n",
    "\n",
    "Escala de preços comparativos com 5 categorias:\n",
    "\n",
    "Posicionamento de Preço em Comparação:\n",
    "\n",
    "- Super Preço:\n",
    "Preço muito abaixo da média para modelos similares.\n",
    "\n",
    "- Bom Preço:\n",
    "Preço abaixo da média para modelos similares.\n",
    "\n",
    "- Preço Justo:\n",
    "Preço em linha com a média para modelos similares.\n",
    "\n",
    "- Preço Elevado:\n",
    "Preço ligeiramente acima da média para modelos similares.\n",
    "\n",
    "- Caro:\n",
    "Preço significativamente acima da média para modelos similares.\n",
    "\n",
    "Essa escala simplificada oferece uma visão rápida e clara do posicionamento de preço em comparação com modelos semelhantes, permitindo que os usuários avaliem rapidamente se o carro está sendo oferecido a um preço atrativo ou se está na faixa mais elevada em termos de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f41b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Function to display histograms for each cluster\n",
    "def define_cluster_histograms(df, target_column, cluster_column, bin_tags):\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for cluster_id in df[cluster_column].unique():\n",
    "        \n",
    "        cluster_data = df[df[cluster_column] == cluster_id]\n",
    "        #display(cluster_data)\n",
    "        \n",
    "        if len(list(dict.fromkeys(cluster_data[target_column])))>=len(bin_tags):\n",
    "\n",
    "            # Calculate histogram center and standard deviation\n",
    "            hist_center = cluster_data[target_column].mean()\n",
    "            std_dev = cluster_data[target_column].std()\n",
    "            \n",
    "            # Define the bin edges based on standard deviation\n",
    "            bin_edges = np.linspace(hist_center - (2*std_dev), hist_center + (2*std_dev), len(bin_tags)-1)\n",
    "\n",
    "#             # Extract the number of bins from the bin_edges list\n",
    "#             num_bins = int(round(len(cluster_data) ** 0.5))\n",
    "\n",
    "#             # Create histogram using Plotly Express\n",
    "#             fig = px.histogram(cluster_data, x=target_column, nbins=num_bins, title='Histogram with {} Equal Sliced Areas'.format(len(bin_tags)))\n",
    "\n",
    "#             # Add vertical lines for bin edges\n",
    "#             for edge in bin_edges:\n",
    "#                 fig.add_shape(\n",
    "#                     type='line',\n",
    "#                     x0=edge,\n",
    "#                     x1=edge,\n",
    "#                     y0=0,\n",
    "#                     y1=100,\n",
    "#                     line=dict(color='red', width=2)\n",
    "#                 )\n",
    "\n",
    "#             fig.show()\n",
    "\n",
    "            # Categorize values into bins\n",
    "            # 1a Bin é a mais extrema em valores superiores, a última Bin é a mais extrema em valores inferiores\n",
    "            cluster_data['bin_category'] = pd.cut(cluster_data[target_column], bins=[-np.inf] + list(bin_edges) + [np.inf], labels=bin_tags)\n",
    "\n",
    "        else:\n",
    "            print('Could not categorize cluster {}'.format(cluster_id))\n",
    "        \n",
    "        clusters.append(cluster_data)            \n",
    "    \n",
    "    return pd.concat(clusters)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Definindo as tags\n",
    "\n",
    "price_tags = ['Super Preço','Bom Preço','Preço Justo','Preço Elevado','Caro']\n",
    "target_column = 'sre_value'\n",
    "\n",
    "priced_clustered_base_v_toML = define_cluster_histograms(clustered_base_v_toML, target_column, 'cluster', price_tags)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "display(priced_clustered_base_v_toML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ddba9",
   "metadata": {},
   "source": [
    "### Exibindo super ofertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0f115",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "super_orefas_df = priced_clustered_base_v_toML[priced_clustered_base_v_toML['bin_category']=='Super Preço']\n",
    "\n",
    "#Retraduzindo os dados mapeados:\n",
    "for col in [c for c in base_v_colunas_mapeadas if c in list(super_orefas_df.columns)]:\n",
    "    \n",
    "    # encontrando o dicionário de mapeamento \n",
    "    mapping_dict = next(item.get(col) for item in base_v_maps if col in item)\n",
    "        \n",
    "    super_orefas_df[col] = super_orefas_df[col].map({v: k for k, v in mapping_dict.items()})\n",
    "\n",
    "    \n",
    "# ReCriando coluna de ano\n",
    "super_orefas_df['sre_year'] = datetime.datetime.now().year-super_orefas_df['sre_age']\n",
    "    \n",
    "super_orefas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547f383",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAMADA\n",
    "def save_mongo_ranking(final_full_sre_ranking,db, collection):\n",
    "    client = MongoClient('mongodb+srv://gabrielaraujon96:D3umaten0ve@carhunt.sweobl3.mongodb.net/?retryWrites=true&w=majority&appName=carhunt')\n",
    "    db = client['carhunt']\n",
    "    collection = db['car_ranking']\n",
    "    data = final_full_sre_ranking.to_dict(orient= 'records')\n",
    "    collection.insert_many(data)\n",
    "    print(\"Dados inserido!\")\n",
    "\n",
    "results_rank_df, CarHuntML_models = ML_ranking_gen2(df_ALL_sre_toML, train_method='arbit')\n",
    "\n",
    "# Model Persistence\n",
    "# Save the best models to disk for later use\n",
    "joblib.dump(CarHuntML_models, 'CarHuntML_models_{}.pkl'.format(datetime.datetime.now().strftime(\"%d%b%Y-%Hh%M\")))    \n",
    "\n",
    "\n",
    "\n",
    "#Retraduzindo os dados mapeados:\n",
    "for col in [c for c in colunas_mapeadas_df_ALL_sre_maps if c in list(results_rank_df.columns)]:\n",
    "\n",
    "    # encontrando o dicionário de mapeamento \n",
    "    mapping_dict = next(item.get(col) for item in df_ALL_sre_maps if col in item)\n",
    "\n",
    "    results_rank_df[col] = results_rank_df[col].map({v: k for k, v in mapping_dict.items()})\n",
    "    \n",
    "    \n",
    "# ReCriando coluna de ano\n",
    "# results_rank_df['sre_year'] = datetime.datetime.now().year-results_rank_df['sre_age']\n",
    "\n",
    "# ReColocanod colunas anteriormente ignoradas na tratativa de ML\n",
    "final_full_sre_ranking = pd.concat([results_rank_df.sort_index(), df_ALL_sre[drop_cols_df_ALL_sre].sort_index()], axis=1)\n",
    "\n",
    "# Ordenando campeões -----\n",
    "final_full_sre_ranking = final_full_sre_ranking.sort_values(\"Ranking_CarHunt\", ascending=False)\n",
    "\n",
    "# Reorganizando as colunas\n",
    "ml_cols=['outlier_flag','predicted_ML_value','price','diff_ml','Ranking_CarHunt']\n",
    "final_full_sre_ranking = final_full_sre_ranking[[c for c in list(final_full_sre_ranking.columns) if c not in ml_cols]+ml_cols]\n",
    "\n",
    "save_mongo_ranking(final_full_sre_ranking, 'carhunt', 'car_ranking')\n",
    "\n",
    "display(final_full_sre_ranking)\n",
    "\n",
    "df_ALL_sre_maps[-1]['body'].items()\n",
    "\n",
    "# HISTORY DO RANKING\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_mongodb_history(records):\n",
    "    try:\n",
    "        # Configurar a conexão com o MongoDB\n",
    "        client = MongoClient('mongodb+srv://gabrielaraujon96:D3umaten0ve@carhunt.sweobl3.mongodb.net/?retryWrites=true&w=majority&appName=carhunt')\n",
    "        db = client['carhunt']\n",
    "        collection = db['car_history']\n",
    "    except Exception as e:\n",
    "            print(f\"Ocorreu um erro ao enviar os dados para o MongoDB: {e}\")\n",
    "            \n",
    "#historico_path = 'historico_ranking.csv'\n",
    "def save_mongo_history_ranking(data,db, collection):\n",
    "    client = MongoClient('mongodb+srv://gabrielaraujon96:D3umaten0ve@carhunt.sweobl3.mongodb.net/?retryWrites=true&w=majority&appName=carhunt')\n",
    "    db = client['carhunt']\n",
    "    collection = db['car_history']\n",
    "    data = historico_ranking.to_dict(orient= 'records')\n",
    "    collection.insert_many(data)\n",
    "\n",
    "\n",
    "if os.path.exists(historico_path):\n",
    "    historico_ranking = pd.read_csv(historico_path)\n",
    "else:\n",
    "    historico_ranking = pd.DataFrame(columns=['brand', 'model', 'city', 'state', 'km', 'transmission', 'body', 'year',\n",
    "                                         'link', 'image', 'source', 'type', 'outlier_flag', 'predicted_ML_value', \n",
    "                                         'price', 'diff_ml', 'Ranking_CarHunt', 'rank_date'])\n",
    "\n",
    "\n",
    "final_full_sre_ranking['rank_date'] = pd.to_datetime('today')\n",
    "\n",
    "# Selecionar os primeiros 50 carros do ranking e adiciona ao histórico\n",
    "add_rank = final_full_sre_ranking.head(50)\n",
    "\n",
    "\n",
    "if 'Ranking_CarHunt' in historico_ranking.columns:\n",
    "    historico_ranking = add_rank[add_rank['Ranking_CarHunt']>8]\n",
    "    #historico_ranking = pd.concat([historico_ranking, add_rank_filtered], ignore_index=True)\n",
    "\n",
    "if 'link' in historico_ranking.columns:\n",
    "    # Adiciona novos carros ao histórico\n",
    "    #historico_ranking = pd.concat([historico_ranking, add_rank_filtered], ignore_index=True)\n",
    "    # Remove duplicatas baseadas na coluna 'link'\n",
    "    historico_ranking = historico_ranking.drop_duplicates(subset=['link'], keep='last')\n",
    "\n",
    "# Ranking_CarHunt em ordem decrescente\n",
    "historico_ranking = historico_ranking.sort_values(by='Ranking_CarHunt', ascending=False)\n",
    "\n",
    "# Salvar o histórico atualizado\n",
    "#historico_ranking.to_csv(historico_path, index=False)\n",
    "\n",
    "    \n",
    "print(\"Histórico atualizado com novos carros.\")\n",
    "\n",
    "save_mongo_history_ranking(historico_ranking, 'carhunt', 'car_history')\n",
    "\n",
    "display(historico_ranking.head(100))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f8733",
   "metadata": {},
   "source": [
    "# Criar Rotina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import schedule\n",
    "\n",
    "# def tarefa_atualizacao():\n",
    "#     final_full_sre_ranking = ML_ranking_gen2()\n",
    "#     atualizar_historico(final_full_sre_ranking)\n",
    "#     print(\"Atualização concluída.\")\n",
    "\n",
    "\n",
    "# schedule.every(1).hours.do(tarefa_atualizacao)\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)\n",
    "\n",
    "\n",
    "# import schedule\n",
    "# import time\n",
    "# def rotina_atualizacao(intervalo_segundos):\n",
    "#     while True:\n",
    "#         print(\"Iniciando atualização...\")\n",
    "#         print(\"DataFrame atual:\")\n",
    "#         # print(final_full_sre_ranking)  # Debugging: Exibe o DataFrame\n",
    "#         final_full_sre_ranking = ML_ranking_gen2(final_full_sre_ranking)\n",
    "#         save_mongo_history_ranking(final_full_sre_ranking)\n",
    "#         print(\"Atualização concluída!\")\n",
    "#         time.sleep(intervalo_segundos)\n",
    "# rotina_atualizacao(3600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
